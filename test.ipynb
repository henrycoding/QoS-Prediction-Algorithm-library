{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from const import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBase(object):\n",
    "    def __init__(self,type_) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.type = type_\n",
    "        assert self.type in [\"rt\",\"tp\",\"user\",\"service\"],f\"类型不符，请在{['rt','tp','user','service']}中选择\"\n",
    "        \n",
    "    def get_row_data(self):\n",
    "        if self.type == \"rt\":\n",
    "            data =  np.loadtxt(RT_MATRIX_DIR)\n",
    "        elif self.type == \"tp\":\n",
    "            data =  np.loadtxt(TP_MATRIX_DIR)\n",
    "        elif self.type == \"user\":\n",
    "            data =  pd.read_csv(USER_DIR,sep=\"\\t\")\n",
    "        elif self.type == \"service\":\n",
    "            data =  pd.read_csv(WS_DIR,sep=\"\\t\")\n",
    "        return data\n",
    "\n",
    "class MatrixDataset(DatasetBase):\n",
    "    def __init__(self,type_) -> None:\n",
    "        super().__init__(type_)\n",
    "    \n",
    "    def get_row_data(self):\n",
    "        data = super().get_row_data()\n",
    "        self.row_n,self.col_n = data.shape\n",
    "        return data\n",
    "\n",
    "    def get_triad(self,nan_symbol = -1):\n",
    "        \"\"\"生成三元组(uid,iid,rate)\n",
    "\n",
    "        Args:\n",
    "            nan_symbol (int, optional): 数据集中用于表示数据缺失的值. Defaults to -1.\n",
    "\n",
    "        Returns:\n",
    "            list[list]: (uid,iid,rate)\n",
    "        \"\"\"\n",
    "        triad_data = []\n",
    "        row_data = self.get_row_data()\n",
    "        if isinstance(row_data,pd.DataFrame):\n",
    "            row_data = row_data.to_numpy()\n",
    "        row_data[row_data == nan_symbol] = 0\n",
    "        non_zero_index_tuple = np.nonzero(row_data)\n",
    "        for uid,iid in zip(non_zero_index_tuple[0],non_zero_index_tuple[1]):\n",
    "            triad_data.append([uid,iid,row_data[uid,iid]])\n",
    "        \n",
    "        triad_data = np.array(triad_data)\n",
    "    \n",
    "        return triad_data\n",
    "\n",
    "    def split_train_test(self,density,nan_symbol = -1,shuffle=True):\n",
    "        traid_data = self.get_triad(nan_symbol)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(shuffle)\n",
    "\n",
    "        train_n =  int(self.row_n * self.col_n * density) # 训练集数量\n",
    "        train_data,test_data = traid_data[:train_n,:],traid_data[train_n:,:]\n",
    "\n",
    "        return train_data,test_data\n",
    "\n",
    "\n",
    "    def get_mini_triad(self,nan_symbol = -1, sample_nums=200):\n",
    "        total_triad_data = self.get_triad(nan_symbol)\n",
    "        return random.sample(total_triad_data,sample_nums)\n",
    "    \n",
    "    def mini_split_train_test(self,density,nan_symbol = -1,shuffle=True):\n",
    "\n",
    "        traid_data = self.get_mini_triad(nan_symbol)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(shuffle)\n",
    "\n",
    "        train_n =  int(self.row_n * self.col_n * density) # 训练集数量\n",
    "        train_data,test_data = traid_data[:train_n,:],traid_data[train_n:,:]\n",
    "\n",
    "        return train_data,test_data\n",
    "\n",
    "import pandas as pd\n",
    "from const import *\n",
    "db = DatasetBase(\"user\")\n",
    "db.get_row_data()[[\"[Latitude]\",\"[Longitude]\"]].to_csv(\"./test2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def _mlp_layer(n,dim,layers=[32,16,8]):\n",
    "    embedding_layer = nn.Embedding(n,dim)\n",
    "    fc_layers = []\n",
    "    for in_size,out_size in zip(layers[:-1],layers[1:]):\n",
    "        fc_layers.append(nn.Linear(in_size,out_size))\n",
    "    return nn.Sequential(\n",
    "        embedding_layer,\n",
    "        *fc_layers\n",
    "    )\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self,n,dim,layers=[32,16,8],output_dim=1) -> None:\n",
    "        \"\"\"一个简单的MLP 特征提取网络\n",
    "\n",
    "        Args:\n",
    "            n ([type]): 用户或者物品的数量\n",
    "            dim ([type]): 特征空间的维度\n",
    "            layers (list, optional): 多层感知机的层数. Defaults to [16,32,16,8].\n",
    "            output_dim (int, optional): 最后输出的维度. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(MLPModel,self).__init__()\n",
    "        self.n = n # \n",
    "        self.latent_dim = dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.n, embedding_dim=self.latent_dim)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        for in_size,out_size in zip(layers[:-1],layers[1:]):\n",
    "            self.fc_layers.append(nn.Linear(in_size,out_size))\n",
    "        \n",
    "        self.fc_output = nn.Linear(layers[-1],output_dim)\n",
    "\n",
    "    def forward(self,idx):\n",
    "        x = self.embedding(idx)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "            x = nn.ReLU()(x)\n",
    "        x = self.fc_output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class QoSNet(nn.Module):\n",
    "    def __init__(self,n_user,n_item,dim,mlp_layers=[32,16,8],qos_layers=[8],output_dims=1) -> None:\n",
    "        super(QoSNet,self).__init__()\n",
    "        self.u_layers = _mlp_layer(n_user,dim,mlp_layers)\n",
    "        self.s_layers = _mlp_layer(n_item,dim,mlp_layers)\n",
    "        d = mlp_layers[-1] * 2\n",
    "        layers = [d] + qos_layers[:]\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for in_size,out_size in zip(layers[:-1],layers[1:]):\n",
    "            self.fc_layers.append(nn.Linear(in_size,out_size))\n",
    "        self.fc_output = nn.Linear(qos_layers[-1],output_dims)\n",
    "\n",
    "    def forward(self,n_idx,s_idx):\n",
    "        u_feature = self.u_layers(n_idx)\n",
    "        s_feature = self.s_layers(s_idx)\n",
    "        x = torch.cat([u_feature,s_feature],dim=1)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "            x = nn.ReLU()(x)\n",
    "        x = self.fc_output(x)\n",
    "        return x,u_feature,s_feature\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD,Adam\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb = SummaryWriter()\n",
    "\n",
    "\n",
    "def train(model,epochs,train_loader,eval_loader,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_total_loss = 0\n",
    "        eval_total_loss = 0\n",
    "        for batch_id,batch in tqdm(enumerate(train_loader)):\n",
    "            user,item,rating = batch[0],batch[1],batch[2]\n",
    "            y_real = rating.reshape(-1,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(user,item)\n",
    "            loss = loss_fn(y_pred,y_real) # must be (1. nn output, 2. target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_total_loss += loss.item()\n",
    "        \n",
    "        loss_per_epoch = train_total_loss/len(train_loader)\n",
    "        train_loss_list.append(loss_per_epoch)\n",
    "        print(f\"Training Epoch:[{epoch}/{epochs}] Loss:{loss_per_epoch:.4f}\")\n",
    "\n",
    "        tb.add_scalar(\"Train Loss\",loss_per_epoch,epoch)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_id,batch in tqdm(enumerate(eval_loader)):\n",
    "                    user,item,rating = batch[0],batch[1],batch[2]\n",
    "                    y_pred = model(user,item)\n",
    "                    y_real = rating.reshape(-1,1)\n",
    "                    loss = loss_func(y_pred,y_real)\n",
    "                    eval_total_loss += loss.item()\n",
    "                loss_per_epoch = eval_total_loss/len(eval_loader)\n",
    "                eval_loss_list.append(loss_per_epoch)\n",
    "                print(f\"Test loss:\",loss_per_epoch) \n",
    "\n",
    "                tb.add_scalar(\"Eval loss\",loss_per_epoch,epoch)\n",
    "    return train_loss_list,eval_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理用户服务信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MatrixDataset,InfoDataset,ToTorchDataset\n",
    "from tqdm import tqdm\n",
    "from models.FedXXX.model import FedXXXModel,Embedding,FedXXXLaunch,FedXXX\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from models.FedXXX.utils import ResNetBasicBlock\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RESULT MODEL:\n",
    "\"\"\"\n",
    "\n",
    "IS_FED = True\n",
    "\n",
    "epochs = 10\n",
    "desnity = 0.05\n",
    "type_ = \"rt\"\n",
    "\n",
    "u_enable_columns = [\"[User ID]\", \"[Country]\"]\n",
    "i_enable_columns = [\"[Service ID]\", \"[Country]\"]\n",
    "\n",
    "def data_preprocess(traid,u_info_obj:InfoDataset,i_info_obj:InfoDataset,is_dtraid=False):\n",
    "    \"\"\"生成d_traid\n",
    "\n",
    "    Args:\n",
    "        traid ([type]): [description]\n",
    "        u_info_obj (InfoDataset): [description]\n",
    "        i_info_obj (InfoDataset): [description]\n",
    "        need_uid (bool, optional): [description]. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    for row in tqdm(traid,desc=\"Data preprocess\"):\n",
    "        uid,iid,rate = int(row[0]),int(row[1]),float(row[2])\n",
    "        u = u_info_obj.query(uid)\n",
    "        i = i_info_obj.query(iid)\n",
    "        r.append([[uid,iid,rate],[u,i,rate]]) if is_dtraid else r.append([uid,iid,rate])\n",
    "    return r\n",
    "\n",
    "fed_data_preprocess = partial(data_preprocess,is_dtraid=True)\n",
    "\n",
    "\n",
    "\n",
    "md = MatrixDataset(type_)\n",
    "u_info = InfoDataset(\"user\",u_enable_columns)\n",
    "i_info = InfoDataset(\"service\",i_enable_columns)\n",
    "train,test = md.split_train_test(desnity)\n",
    "\n",
    "user_params = {\n",
    "    \"type_\":\"stack\", # embedding层整合方式 stack or cat\n",
    "    \"embedding_nums\":u_info.embedding_nums,# 每个要embedding的特征的总个数\n",
    "    \"embedding_dims\":[16,16],\n",
    "    \"in_size\":16, # embedding后接一个全连阶层在进入resnet\n",
    "    \"blocks_sizes\":[16,8], # 最后的输出是8\n",
    "    \"deepths\":[2],\n",
    "    \"activation\":nn.ReLU,\n",
    "    \"block\":ResNetBasicBlock\n",
    "}\n",
    "\n",
    "item_params = {\n",
    "    \"type_\":\"stack\", # embedding层整合方式 stack or cat\n",
    "    \"embedding_nums\":i_info.embedding_nums,# 每个要embedding的特征的总个数\n",
    "    \"embedding_dims\":[16,16],\n",
    "    \"in_size\":16,\n",
    "    \"blocks_sizes\":[16,8], # item最后的输出是8\n",
    "    \"deepths\":[2],\n",
    "    \"activation\":nn.ReLU,\n",
    "    \"block\":ResNetBasicBlock\n",
    "}\n",
    "\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "train_data = fed_data_preprocess(train,u_info,i_info)\n",
    "test_data = fed_data_preprocess(test,u_info,i_info)\n",
    "\n",
    "model = FedXXXLaunch(train_data,user_params,item_params,[16],loss_fn,1,nn.ReLU)\n",
    "# model = FedXXX(user_params, item_params, [16], output_dim=1, activation=nn.ReLU)\n",
    "model.fit(epochs, test_d_traid=test_data, lr=0.001)\n",
    "\n",
    "# from utils.model_util import traid_to_matrix\n",
    "# traid_to_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(1,10)\n",
    "b = np.arange(11,20)\n",
    "np.stack([a,b])\n",
    "\n",
    "class B:\n",
    "    def __init__(self,vec) -> None:\n",
    "        self.vec = vec\n",
    "\n",
    "class A:\n",
    "    def __init__(self) -> None:\n",
    "        self.user_vec = np.array([1,2,3,4,5])\n",
    "    \n",
    "    def to_b(self):\n",
    "        return B(self.user_vec)\n",
    "\n",
    "a = A()\n",
    "b = a.to_b()\n",
    "b.vec[0] = 10\n",
    "\n",
    "a.user_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape([1,2,3],(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from utils.preprocess import min_max_scaler,l2_norm,z_score\n",
    "\n",
    "\n",
    "#创建一组特征数据，每一行标识一个样本，每一列标识一个特征\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "min_max,_ = z_score(X_train)\n",
    "lis = [1,2,3,4]\n",
    "test = np.array(lis,dtype=np.float)\n",
    "# data = _.transform(test)\n",
    "# print(data)\n",
    "print(test)\n",
    "# test = np.reshape([1,2,3],(1,-1))\n",
    "_.inverse_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MatrixDataset\n",
    "from utils.evaluation import mae, mse, rmse\n",
    "from utils.model_util import freeze_random\n",
    "\n",
    "from models.FedMF import Clients, Server\n",
    "from models.FedMF.model import FedMF\n",
    "\"\"\"\n",
    "RESULT FedMF: \n",
    "1000epoch\n",
    "Density:0.05,type:rt,mae:0.6090604947629363,mse:2.3006280931616536,rmse:1.5167821508580768\n",
    "Density:0.1,type:rt,mae:0.5071641017174705,mse:1.7203263210368958,rmse:1.3116121076891962\n",
    "Density:0.15,type:rt,mae:0.46475316376452325,mse:1.4854062714808631,rmse:1.2187724445034287\n",
    "Density:0.2,type:rt,mae:0.43765304163567553,mse:1.3690546770840173,rmse:1.1700660994508034\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "freeze_random()  # 冻结随机数 保证结果一致\n",
    "\n",
    "for density in [0.05]:\n",
    "\n",
    "    # 1\n",
    "    type_ = \"tp\"\n",
    "    latent_dim = 8\n",
    "    lr = 0.001\n",
    "    lambda_ = 0.1\n",
    "    epochs = 1000\n",
    "\n",
    "    # 2\n",
    "    # type_ = \"rt\"\n",
    "    # latent_dim = 12\n",
    "    # lr = 0.0005\n",
    "    # lambda_ = 0.1\n",
    "    # epochs = 2000\n",
    "\n",
    "    md_data = MatrixDataset(type_)\n",
    "    train_data, test_data = md_data.split_train_test(density,\n",
    "                                                     normalize_type=\"z_score\")\n",
    "    clients = Clients(train_data, md_data.row_n, latent_dim)\n",
    "\n",
    "    server = Server(md_data.col_n, latent_dim)\n",
    "\n",
    "    print(md_data.scaler)\n",
    "    test = np.array([1,2,3,4,5],dtype=np.float)\n",
    "    md_data.scaler.inverse_transform(test)\n",
    "    # mf = FedMF(server, clients)\n",
    "    # mf.fit(epochs, lambda_, lr, test_data, scaler=md_data.scaler)\n",
    "    # y, y_pred = mf.predict(test_data)\n",
    "\n",
    "    # mae_ = mae(y, y_pred)\n",
    "    # mse_ = mse(y, y_pred)\n",
    "    # rmse_ = rmse(y, y_pred)\n",
    "\n",
    "    # print(f\"Density:{density},type:{type_},mae:{mae_},mse:{mse_},rmse:{rmse_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "Wtrue = np.random.rand(40, 5)\n",
    "Htrue = np.random.rand(5, 10)\n",
    "\n",
    "V0 = Wtrue @ Htrue\n",
    "print(V0.shape)\n",
    "\n",
    "m,n = V0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 6\n",
    "W = np.random.rand(m,r)\n",
    "H = np.random.rand(r,n)\n",
    "np.size(W,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "np.corrcoef([0.5,0.4],[0.5,0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "import torch\n",
    "from models.base import FedModelBase\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from utils.evaluation import mae, mse, rmse\n",
    "from utils.model_util import load_checkpoint, save_checkpoint\n",
    "from utils.mylogger import TNLog\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_user,\n",
    "                 n_item,\n",
    "                 dim,\n",
    "                 layers=[8],\n",
    "                 output_dim=1) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_user ([type]): 用户数量\n",
    "            n_item ([type]): 物品数量\n",
    "            dim ([type]): 特征空间的维度\n",
    "            layers (list, optional): 多层感知机的层数. Defaults to [16,32,16,8].\n",
    "            output_dim (int, optional): 最后输出的维度. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = n_user\n",
    "        self.num_items = n_item\n",
    "        self.latent_dim = dim\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users,\n",
    "                                           embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items,\n",
    "                                           embedding_dim=self.latent_dim)\n",
    "\n",
    "\n",
    "        self.my_layer = nn.Sequential(OrderedDict({\n",
    "            \"my_layer\":nn.Linear(layers[-1],layers[-1])\n",
    "        }))\n",
    "        \n",
    "        self.fc_output = nn.Linear(layers[-1], output_dim)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_embedding = self.embedding_user(user_idx)\n",
    "        item_embedding = self.embedding_item(item_idx)\n",
    "        x = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "            x = nn.ReLU()(x)\n",
    "        x = self.my_layer(x)\n",
    "        x = self.fc_output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def show_params(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        print(param)\n",
    "\n",
    "\n",
    "mlp_center = MLP(10,10,8)\n",
    "mlp1 = MLP(10,10,8)\n",
    "\n",
    "\n",
    "show_params(mlp1)\n",
    "# show_params(mlp_center)\n",
    "\n",
    "print(\"=====\")\n",
    "\n",
    "state_dic = mlp_center.state_dict()\n",
    "\n",
    "\n",
    "# 拿到中心模型的参数,把用本地模型的参数去替换\n",
    "\n",
    "for name,param in mlp1.named_parameters():\n",
    "    if \"my_layer\" in name:\n",
    "        state_dic[name] = param\n",
    "        print(param)\n",
    "\n",
    "mlp1.load_state_dict(state_dic)\n",
    "\n",
    "show_params(mlp1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"/Users/wenzhuo/Desktop/test.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data, index=[1]).T\n",
    "df.to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fe91bd54e4eab9fd37ff994b75a751127066e152ab17ebb6cd473f82f3fc78b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
